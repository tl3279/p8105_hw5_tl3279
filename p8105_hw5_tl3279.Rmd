---
title: "p8105_hw5_tl3279"
author: "Tianqi Li"
date: "2024-11-11"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(readr)
```

## Problem 1

Setup the function to detect duplicate for birthday.
```{r}
bday_sim = function(n) {

  bdays = sample(1:365, size = n, replace = TRUE)
  
  duplicate = length(unique(bdays)) < n
  
  return(duplicate)
}
```

Run this function 10000 times for each group size between 2 and 50. Make a plot showing the probability as a function of group size, and comment on your results
```{r}
sim_res = 
  expand_grid(
    n = c(2:50),
    iter = 1:10000
  ) |>
  mutate(res = map_lgl(n,bday_sim)) |>
  group_by(n) |>
  summarize(prob = mean(res))

sim_res |>
  ggplot(aes(x = n, y = prob)) +
  geom_line() +
  labs(x = "Group Size", y = "Probability of Shared Birthday")
```
The probability of at least two people sharing a birthday increases as the group size grows. The probability starts off close to zero for very small group sizes but increases rapidly. As the group size approaches 23, the  probability increased to about 50%. For larger group sizes, the probability approaches 1, indicating near certainty that at least two people will share a birthday.

## Problem 2

Set up the function
```{r}
exp_sim = function(mu) {
  data = rnorm(30, mean = mu, sd = 5)
  tidy_result = 
    broom::tidy(t.test(data, mu = 0)) |>
    select(estimate, p.value)
  return(tidy_result)
}

exp_df = 
  expand_grid(
    mu = c(0:6),
    iter = 1:5000
  ) |>
  mutate(result = map(mu, exp_sim)) |>
  unnest(result)
```

Power vs true mean plot
```{r}
exp_df |>
  group_by(mu) |>
  summarize(power = mean(p.value < 0.05)) |>
  ggplot(aes(x = mu, y = power)) +
  geom_bar(stat = "identity") +
  labs(
    x = "True Value of mu",
    y = "Power (Proportion of Null Rejections)",
    title = "Power of the Test vs True Value of mu"
  ) +
  theme_minimal()
```

The plot shows a strong positive association between effect size (true value of mu) and the power of the test. As the effect size increases, the power also increases, indicating a higher probability of correctly rejecting the null hypothesis when it is false. For small effect sizes (e.g., mu = 1), the power is low, meaning thereâ€™s a higher chance of failing to detect a true effect. In contrast, for larger effect sizes (e.g., mu = 4 and above), the power approaches 1, meaning the test reliably detects the difference from zero.

Average estimate vs. true mean
```{r}
exp_df |>
  group_by(mu) |>
  summarize(
    avg_est = mean(estimate),
    avg_est_rej = mean(estimate[p.value < 0.05])
  )|>
  ggplot(aes(x = mu, y = avg_est)) +
  geom_line()+
  labs(
    x = "True Value of mu",
    y = "Average Estimate of mu"
  ) +
  theme_minimal()
```

Add the average for rejected to the graph
```{r}
exp_df |>
  group_by(mu) |>
  summarize(
  avg_est = mean(estimate),
  avg_est_rej = mean(estimate[p.value < 0.05])
  )|>
  ggplot(aes(x = mu)) +
  geom_line(aes(y = avg_est, color = "All Samples", linetype = "All Samples"), size = 1) +
  geom_line(aes(y = avg_est_rej, color = "Rejected Null", linetype = "Rejected Null"), size = 1) +
  labs(
    x = "True Value of mu",
    y = "Average Estimate of mu",
    color = "Legend", 
    linetype = "Legend"
  ) +
  scale_color_manual(values = c("All Samples" = "blue", "Rejected Null" = "red")) +
  scale_linetype_manual(values = c("All Samples" = "solid", "Rejected Null" = "dashed")) +
  theme_minimal() +
  theme(legend.position = "right")
```
The sample average of mu across tests where the null is rejected is not approximately equal to the true value of mu, particularly for smaller mu values. This is due to selection bias: when only considering cases where the null is rejected, we tend to select samples where the observed mean 
mu is farther from zero, resulting in an upward bias. As mu increases, the power of the test improves, reducing this bias, and the estimates more closely align with the true mu.

## Problem 3
```{r message = FALSE}
data_url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicide_data <- read_csv(data_url)
```
The raw data from The Washington Post's homicide dataset contains information on individual homicide cases across 50 large U.S. cities.It has a total of 52179 observations over 12 variables. Each row represents a unique case and includes details such as the date of the homicide, victim demographics (name, race, age, sex, and ethnicity), the disposition of the case (e.g., "Closed by arrest," "Open/No arrest"), and geographic information (city, state, latitude, and longitude). 

Create the city_state variable and analysis
```{r}
total_df = 
  homicide_data |>
  mutate(
    city_state = paste(city, state, sep = ","),
    unsolved = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 1, 0)
    ) |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved)
  )
```

Prop.test for Baltimore, MD
```{r}
baltimore_test =
  total_df |>
  filter(city_state == "Baltimore,MD") |>
  with(prop.test(unsolved_homicides, total_homicides))
baltimore_summary = 
  broom::tidy(baltimore_test) |>
  select(estimate, conf.low, conf.high)|>
  print()
```

Prop.test for all cities
```{r warning = FALSE}
city_tests = 
  total_df |>
  mutate(
    test_result = purrr::map2(unsolved_homicides, total_homicides, 
                       ~prop.test(.x, .y) |> broom::tidy())
  ) |>
  unnest(test_result) |>
  select(city_state, estimate, conf.low, conf.high)
```

```{r, fig.width=8, fig.height=8}
city_tests |>
  arrange(desc(estimate)) |>
  mutate(city_state = factor(city_state, levels = city_state)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.25, color = "darkgray") +
  labs(
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) +
  coord_flip() +
  theme_minimal()
```

